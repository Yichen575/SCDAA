{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Galerkin Method: https://arxiv.org/abs/1708.07469\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from lib.options import BaseOption\n",
    "\n",
    "class DGM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(DGM_Layer, self).__init__()\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "            \n",
    "\n",
    "        self.gate_Z = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_G = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_R = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_H = self.layer(dim_x+dim_S, dim_S)\n",
    "            \n",
    "    def layer(self, nIn, nOut):\n",
    "        l = nn.Sequential(nn.Linear(nIn, nOut), self.activation)\n",
    "        return l\n",
    "    \n",
    "    def forward(self, x, S):\n",
    "        x_S = torch.cat([x,S],1)\n",
    "        Z = self.gate_Z(x_S)\n",
    "        G = self.gate_G(x_S)\n",
    "        R = self.gate_R(x_S)\n",
    "        \n",
    "        input_gate_H = torch.cat([x, S*R],1)\n",
    "        H = self.gate_H(input_gate_H)\n",
    "        \n",
    "        output = ((1-G))*H + Z*S\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net_DGM(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(Net_DGM, self).__init__()\n",
    "\n",
    "        self.dim = dim_x\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(dim_x+1, dim_S), self.activation)\n",
    "\n",
    "        self.DGM1 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM2 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM3 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "\n",
    "        self.output_layer = nn.Linear(dim_S, 1)\n",
    "\n",
    "    def forward(self,t,x):\n",
    "        tx = torch.cat([t,x], 1)\n",
    "        S1 = self.input_layer(tx)\n",
    "        S2 = self.DGM1(tx,S1)\n",
    "        S3 = self.DGM2(tx,S2)\n",
    "        S4 = self.DGM3(tx,S3)\n",
    "        output = self.output_layer(S4)\n",
    "        return output\n",
    "\n",
    "def get_gradient(output, x):\n",
    "    grad = torch.autograd.grad(output, x, grad_outputs=torch.ones_like(output), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    return grad\n",
    "\n",
    "def get_laplacian(grad, x):\n",
    "    hess_diag = []\n",
    "    for d in range(x.shape[1]):\n",
    "        v = grad[:,d].view(-1,1)\n",
    "        grad2 = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(v), only_inputs=True, create_graph=True, retain_graph=True)[0]\n",
    "        hess_diag.append(grad2[:,d].view(-1,1))    \n",
    "    hess_diag = torch.cat(hess_diag,1)\n",
    "    laplacian = hess_diag.sum(1, keepdim=True)\n",
    "    return laplacian\n",
    "\n",
    "\n",
    "class PDE_DGM_BlackScholes(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int, hidden_dim: int, mu:float, sigma: float, ts: torch.Tensor=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.net_dgm = Net_DGM(d, hidden_dim, activation='Tanh')\n",
    "        self.ts = ts\n",
    "\n",
    "    def fit(self, max_updates: int, batch_size: int, option, device):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.net_dgm.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = (10000,),gamma=0.1)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=max_updates)\n",
    "        for it in range(max_updates):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_domain = 0.5 + 2*torch.rand(batch_size, self.d, device=device, requires_grad=True)\n",
    "            t0, T = self.ts[0], self.ts[-1]\n",
    "            t = t0 + T*torch.rand(batch_size, 1, device=device, requires_grad=True)\n",
    "            u_of_tx = self.net_dgm(t, input_domain)\n",
    "            grad_u_x = get_gradient(u_of_tx,input_domain)\n",
    "            grad_u_t = get_gradient(u_of_tx, t)\n",
    "            laplacian = get_laplacian(grad_u_x, input_domain)\n",
    "            target_functional = torch.zeros_like(u_of_tx)\n",
    "            pde = grad_u_t + torch.sum(self.mu*input_domain.detach()*grad_u_x,1,keepdim=True) + 0.5*self.sigma**2 * laplacian - self.mu * u_of_tx\n",
    "            MSE_functional = loss_fn(pde, target_functional)\n",
    "            \n",
    "            input_terminal = 0.5 + 2*torch.rand(batch_size, self.d, device=device, requires_grad=True)\n",
    "            t = torch.ones(batch_size, 1, device=device) * T\n",
    "            u_of_tx = self.net_dgm(t, input_terminal)\n",
    "            target_terminal = option.payoff(input_terminal)\n",
    "            MSE_terminal = loss_fn(u_of_tx, target_terminal)\n",
    "\n",
    "            loss = MSE_functional + MSE_terminal\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if it%10 == 0:\n",
    "                pbar.update(10)\n",
    "                pbar.write(\"Iteration: {}/{}\\t MSE functional: {:.4f}\\t MSE terminal: {:.4f}\\t Total Loss: {:.4f}\".format(it, max_updates, MSE_functional.item(), MSE_terminal.item(), loss.item()))\n",
    "\n",
    "    def sdeint(self, ts, x0, antithetic = False):\n",
    "        \"\"\"\n",
    "        Euler scheme to solve the SDE.\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts: troch.Tensor\n",
    "            timegrid. Vector of length N\n",
    "        x0: torch.Tensor\n",
    "            initial value of SDE. Tensor of shape (batch_size, d)\n",
    "        brownian: Optional. \n",
    "            torch.tensor of shape (batch_size, N, d)\n",
    "        Note\n",
    "        ----\n",
    "        I am assuming uncorrelated Brownian motion\n",
    "        \"\"\"\n",
    "        if antithetic:\n",
    "            x = torch.cat([x0.unsqueeze(1), x0.unsqueeze(1)], dim=0)\n",
    "        else:\n",
    "            x = x0.unsqueeze(1)\n",
    "        batch_size = x.shape[0]\n",
    "        device = x.device\n",
    "        brownian_increments = torch.zeros(batch_size, len(ts), self.d, device=device)\n",
    "        for idx, t in enumerate(ts[1:]):\n",
    "            h = ts[idx+1]-ts[idx]\n",
    "            if antithetic:\n",
    "                brownian_increments[:batch_size//2,idx,:] = torch.randn(batch_size//2, self.d, device=device)*torch.sqrt(h)\n",
    "                brownian_increments[-batch_size//2:,idx,:] = -brownian_increments[:batch_size//2, idx, :].clone()\n",
    "            else:\n",
    "                brownian_increments[:,idx,:] = torch.randn(batch_size, self.d, device=device)*torch.sqrt(h)\n",
    "            x_new = x[:,-1,:] + self.mu*x[:,-1,:]*h + self.sigma*x[:,-1,:]*brownian_increments[:,idx,:]\n",
    "            x = torch.cat([x, x_new.unsqueeze(1)],1)\n",
    "        return x, brownian_increments\n",
    "\n",
    "    \n",
    "    def unbiased_price(self, ts: torch.Tensor, x0:torch.Tensor, option: BaseOption, MC_samples: int, ):\n",
    "        \"\"\"\n",
    "        We calculate an unbiased estimator of the price at time t=0 (for now) using Monte Carlo, and the stochastic integral as a control variate\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts: troch.Tensor\n",
    "            timegrid. Vector of length N\n",
    "        x0: torch.Tensor\n",
    "            initial value of SDE. Tensor of shape (1, d)\n",
    "        option: object of class option to calculate payoff\n",
    "        MC_samples: int\n",
    "            Monte Carlo samples\n",
    "        \"\"\"\n",
    "        assert x0.shape[0] == 1, \"we need just 1 sample\"\n",
    "        x0 = x0.repeat(MC_samples, 1)\n",
    "        with torch.no_grad():\n",
    "            x, brownian_increments = self.sdeint(ts, x0)\n",
    "        payoff = option.payoff(x[:,-1,:]) # (batch_size, 1)\n",
    "        device = x.device\n",
    "        batch_size = x.shape[0]\n",
    "        t = ts.reshape(1,-1,1).repeat(batch_size,1,1)\n",
    "        tx = torch.cat([t,x],2) # (batch_size, L, dim+1)\n",
    "        Z = []\n",
    "        for idt, t in enumerate(ts[:-1]):\n",
    "            tt = torch.ones(batch_size, 1, device=device)*t\n",
    "            tt.requires_grad_(True)\n",
    "            xx = x[:,idt,:].requires_grad_(True)\n",
    "            Y = self.net_dgm(tt, xx)\n",
    "            Z.append(get_gradient(Y,xx))\n",
    "        Z = torch.stack(Z, 1)\n",
    "\n",
    "        stoch_int = 0\n",
    "        for idx,t in enumerate(ts[:-1]):\n",
    "            discount_factor = torch.exp(-self.mu*t)\n",
    "            stoch_int += discount_factor * torch.sum(Z[:,idx,:]*brownian_increments[:,idx,:], 1, keepdim=True)\n",
    "        mc = torch.exp(-self.mu*ts[-1])*payoff\n",
    "        cv = stoch_int\n",
    "        cv_mult = torch.mean((mc-mc.mean())*(cv-cv.mean())) / cv.var() # optimal multiplier. cf. Belomestny book\n",
    "        return mc, mc - cv_mult*stoch_int # stoch_int has expected value 0, thus it doesn't add any bias to the MC estimator, and it is correlated with payoff\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f695691c7675195da2229e392149ef068beb920cff0b2fc3ea04696403466e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
