{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee37bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c835c5",
   "metadata": {},
   "source": [
    "## Exercise 1 Linear quadratic regulator\n",
    "### 1.1\n",
    "Write a class which:\n",
    "- Can be initialised with the matrices specifying the LQR problem and T > 0.\n",
    "- Has a method which will solve (approximate) the associated Ricatti ODE on a time grid which is an input (numpy array or torch tensor).\n",
    "- Has a method that, given one torch tensor of dimension batch size (for time) and another torch tensor of dimension batch size × 1 × 2 (for space), will return a torch tensor of dimension batch size × 1 with entries being the control problem value v(t, x) for each t, x in the batch (for x two dimensional).\n",
    "- Has a method that, given one torch tensor of dimension batch size (for time) and another torch tensor of dimension batch size × 1 × 2 (for space), will return a torch tensor of dimension batch size × 2 with entries being the Markov control function for each t, x in the batch (for x two dimensional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38be2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to solve Ricatti ODE\n",
    "def matrix_riccati_ode(t, S, H, M, D_inv, C):\n",
    "    S = S.reshape((H.shape[0], H.shape[1]))\n",
    "    dS_dt = -2 * H.T @ S + S @ M @ D_inv @ M.T @ S - C\n",
    "    return dS_dt.reshape(-1)\n",
    "\n",
    "class LQR:\n",
    "    #This is the constructor method that initializes the LQR object with the necessary parameters.\n",
    "    #H, M, sigma, C, R, D, and T are all input parameters to the constructor method.\n",
    "    def __init__(self, H, M, sigma, C, R, D, T):\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.sigma = sigma\n",
    "        self.C = C\n",
    "        self.R = R\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "\n",
    "    def solve(self, t, t_eval = None):\n",
    "        #This method computes the solution to the matrix Riccati differential equation\n",
    "        D_inv = np.linalg.inv(self.D)\n",
    "        if t == self.T:\n",
    "            return None, None\n",
    "        if t is None:\n",
    "            t = 0\n",
    "        if t_eval is None:\n",
    "            t_eval = np.linspace(self.T, t, 101)\n",
    "        sol = solve_ivp(matrix_riccati_ode, (self.T, t), self.R.reshape(-1), t_eval = t_eval, method='RK45', args=(self.H, self.M, D_inv, self.C))\n",
    "        return sol.t, sol.y.reshape((-1, self.H.shape[0], self.H.shape[1]))  # Reshape solution to matrix form\n",
    "\n",
    "    def getValue(self, ts, data):\n",
    "        #This method computes the value function for a given set of times and data.\n",
    "        batch_size = data.shape[0]\n",
    "        values = torch.zeros((batch_size, 1))\n",
    "        for i in range(batch_size):\n",
    "            t = ts[i]\n",
    "            x = data[i][0].numpy()\n",
    "            n = 101\n",
    "            ts, sols = self.solve(t, np.linspace(self.T, t, n))\n",
    "            values[i][0] = x.T @ sols[-1] @ x\n",
    "            for sol in sols:\n",
    "                values[i][0] += abs(self.T - t) / n * np.trace(self.sigma @ self.sigma.T @ sol)\n",
    "        return values\n",
    "\n",
    "    def getMarkovControl(self, ts, data):\n",
    "        #This method computes the optimal control input for a given set of times ts and data data.\n",
    "        batch_size = data.shape[0]\n",
    "        values = torch.zeros((batch_size, 2))\n",
    "        for i in range(batch_size):\n",
    "            t = ts[i]\n",
    "            x = data[i][0].numpy()\n",
    "            n = 101\n",
    "            ts, sols = self.solve(t, np.linspace(self.T, t, n))\n",
    "            values[i] = torch.from_numpy(-self.D @ self.M.T @ sols[-1] @ x)\n",
    "        return values\n",
    "\n",
    "    \n",
    "    def runMonteCarlo(self, num_steps, num_samples, x):\n",
    "        #This method runs a Monte Carlo simulation to estimate the expected total cost from a given time to the final time T.\n",
    "        res = []\n",
    "        dt = self.T / num_steps\n",
    "        ts = np.linspace(0, self.T, num_steps)\n",
    "        for i in range(num_samples):\n",
    "            cur_x = np.array(x).reshape(-1,1)\n",
    "            for j in range(num_steps - 1):\n",
    "                increments = np.random.normal(0, 1, (1,2)).T\n",
    "                _, S_tn = self.solve(ts[j])\n",
    "                S_tn = S_tn[-1]\n",
    "                cur_x = cur_x + dt * (self.H @ cur_x - self.M @ self.D @ S_tn @ cur_x) + self.sigma @ increments\n",
    "            res.append(cur_x)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b2a608",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "H = np.array([[0, 1], [0, 0]])\n",
    "M = np.array([[1, 0], [0, 1]])\n",
    "D = np.array([[1, 0], [0, 1]])\n",
    "T = 10\n",
    "R = np.array([[1, 0], [0, 1]])\n",
    "C = np.array([[0, 0], [0, 0]])\n",
    "sigma = np.array([[0.2, 0], [0, 0.4]])\n",
    "lqr = LQR(H, M, sigma, C, R, D, T)\n",
    "\n",
    "t, p = lqr.solve(1, [10, 9, 8,7,6,5,4,3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36d9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.tensor([1, 2])\n",
    "x = torch.tensor([[[2,1]], [[3,2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f62440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b102092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5519],\n",
       "        [22.9790]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lqr.getValue(ts, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88e215c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3080, -0.3024],\n",
       "        [-4.5970, -4.5894]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lqr.getMarkovControl(ts, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b70289",
   "metadata": {},
   "source": [
    "From the above code, we define the class LQR with attribute 'H, M, sigma, C, R, D, T', which is mentioned in the definition of LQR. The class has three methods - solve, getValue, getMarkovControl. 'Solve' will solve the associated Ricatti ODE on a time grid. 'getValue' will return the control problem value $v(t,x)$. 'getMarkovControl' will return the Markov control function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5f4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c037c21",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "We can choose MSE for error measure and we write following codes to show log-log plot.\n",
    "\n",
    "Run a Monte Carlo simulation of the system with the optimal solution you have obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd40dcd",
   "metadata": {},
   "source": [
    "### 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6103198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set number of Monte Carlo samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Set different numbers of time steps to simulate\n",
    "num_steps_list = [1, 10, 50, 100, 500, 1000, 5000]\n",
    "\n",
    "# Initialize list to store MSE for each number of time steps\n",
    "mse_list = []\n",
    "\n",
    "# initial value of x\n",
    "x = [1, 2]\n",
    "\n",
    "# Loop over different numbers of time steps and compute MSE\n",
    "for num_steps in num_steps_list:\n",
    "    # Run Monte Carlo simulation\n",
    "    # Here, we assume that the simulation returns the estimated value function as a numpy array\n",
    "    # and the true value function as a dictionary with state-value pairs\n",
    "    x_rets = lqr.runMonteCarlo(num_steps, num_samples, x)\n",
    "    _, x_true = lqr.solve(0)\n",
    "    x_true = x_true[-1]\n",
    "    \n",
    "    # Compute MSE\n",
    "    mse = np.mean([np.linalg.norm(x_ret - x_true)**2 for x_ret in x_rets])\n",
    "    \n",
    "    # Append MSE to list\n",
    "    mse_list.append(mse)\n",
    "\n",
    "\n",
    "plt.loglog(num_steps_list, mse_list, '-o')\n",
    "plt.xlabel('Number of time steps')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7824ad",
   "metadata": {},
   "source": [
    "### 1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb79410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of Monte Carlo samples\n",
    "num_samples_list = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "# Set different numbers of time steps to simulate\n",
    "num_steps = 5000\n",
    "\n",
    "# Initialize list to store MSE for each number of time steps\n",
    "mse_list = []\n",
    "\n",
    "# initial value of x\n",
    "x = [1, 2]\n",
    "\n",
    "# Loop over different numbers of time steps and compute MSE\n",
    "for num_samples in num_samples_list:\n",
    "    # Run Monte Carlo simulation\n",
    "    # Here, we assume that the simulation returns the estimated value function as a numpy array\n",
    "    # and the true value function as a dictionary with state-value pairs\n",
    "    x_rets = lqr.runMonteCarlo(num_steps, num_samples, x)\n",
    "    _, x_true = lqr.solve(0)\n",
    "    x_true = x_true[-1]\n",
    "    \n",
    "    # Compute MSE\n",
    "    mse = np.mean([np.linalg.norm(x_ret - x_true)**2 for x_ret in x_rets])\n",
    "    \n",
    "    # Append MSE to list\n",
    "    mse_list.append(mse)\n",
    "\n",
    "\n",
    "plt.loglog(num_steps_list, mse_list, '-o')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0d63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e147471f",
   "metadata": {},
   "source": [
    "## Exercise 2 Supervised learning, checking the NNs are good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43475ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DGM_Layer(nn.Module):\n",
    "    \"\"\" \n",
    "    Defines a PyTorch module DGM_Layer which implements a single layer of a deep gated network.\n",
    "    The activation parameter specifies the activation function to be used in the layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "\n",
    "\n",
    "        super(DGM_Layer, self).__init__()\n",
    "        \n",
    "\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "            \n",
    "        \n",
    "        # The module has four gates\n",
    "        # Z gate (update gate): controls how much of the previous state to keep and how much of the new input to include in the current state. \n",
    "        self.gate_Z = self.layer(dim_x+dim_S, dim_S)\n",
    "        # G gate (reset gate): controls how much of the previous state to forget and how much of the new input to remember. \n",
    "        self.gate_G = self.layer(dim_x+dim_S, dim_S)\n",
    "        # R gate (reset signal): controls how much of the input to use to modify the current state.\n",
    "        self.gate_R = self.layer(dim_x+dim_S, dim_S)\n",
    "        # H gate (hidden state): produces a candidate hidden state that can be added to the current state. \n",
    "        self.gate_H = self.layer(dim_x+dim_S, dim_S)\n",
    "            \n",
    "    def layer(self, nIn, nOut):\n",
    "        l = nn.Sequential(nn.Linear(nIn, nOut), self.activation)\n",
    "        return l\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, x, S):\n",
    "        # concatenates  input 'x' and the hidden state 'S' along the feature dimension\n",
    "        x_S = torch.cat([x,S],1)\n",
    "        \n",
    "        \n",
    "        # passes the concatenated tensor through each of the four gates. \n",
    "        Z = self.gate_Z(x_S) # how much of the new state to keep\n",
    "        G = self.gate_G(x_S) # how much proportion of the previous state to keep\n",
    "        R = self.gate_R(x_S) # the proportion of the input to use to modify the current state.\n",
    "\n",
    "        \n",
    "        input_gate_H = torch.cat([x, S*R],1) # 'R*S' is modified state\n",
    "        H = self.gate_H(input_gate_H)  #  candidate hidden state, and is the amount of information from the current input to add to the new state\n",
    "\n",
    "        \n",
    "        # The resulting tensors are then combined to compute the new hidden state S for the next time step, which is returned as output.\n",
    "        output = ((1-G))*H + Z*S\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net_DGM(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        \"\"\"\n",
    "        dim_x is the dimension of the input tensor,\n",
    "        dim_S is the number of hidden units in each of the three DGM_Layer objects,\n",
    "        activation is the activation function used in the hidden layers, which can be ReLU, Tanh, Sigmoid, or LogSigmoid.\n",
    "        The constructor initializes the class variables using the arguments, and creates the layers of the neural network.\n",
    "        \"\"\"\n",
    "        super(Net_DGM, self).__init__()\n",
    "      \n",
    "        self.dim = dim_x\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(dim_x+1, dim_S), self.activation)\n",
    "        # The input_layer is a linear layer followed by an activation function, which takes a concatenation of t and x as input\n",
    "        self.DGM1 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM2 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM3 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        \"\"\"\n",
    "        DGM1, DGM2, and DGM3 are three layers of the Deep Gaussian Process (DGP) model. \n",
    "        They are initialized with \n",
    "        dim_x+1 as input dimension (where the +1 accounts for the time input t), \n",
    "        dim_S as output dimension, and the activation function specified in the constructor.\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(dim_S, 1)\n",
    "        # The output_layer is a linear layer that takes the output from the last DGM layer as input and produces a single output value.\n",
    "        \n",
    "    def forward(self,t,x):\n",
    "        # The forward method defines the forward pass of the neural network, which takes two input tensors t and x and returns a single output tensor.\n",
    "        # t and x are concatenated along the second dimension using torch.cat to create a single input tensor.\n",
    "        tx = torch.cat([t,x], 1)\n",
    "\n",
    "        # This input tensor is then passed through the input_layer to obtain S1, which is then passed through DGM1, DGM2, and DGM3 to obtain S4.\n",
    "        S1 = self.input_layer(tx)\n",
    "        S2 = self.DGM1(tx,S1)\n",
    "        S3 = self.DGM2(tx,S2)\n",
    "        S4 = self.DGM3(tx,S3)\n",
    "\n",
    "        # Finally, S4 is passed through the output_layer to obtain the output of the neural network, which is returned.\n",
    "        output = self.output_layer(S4)\n",
    "        return output\n",
    "\n",
    "\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ddf64",
   "metadata": {},
   "source": [
    "### Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e39dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/var/folders/30/5npdvf4d7mb7d1df8ndd9dmm0000gn/T/ipykernel_22462/2836287028.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(lqr.getValue(torch.tensor([t]), torch.tensor([[x]], dtype=torch.float32)))\n",
      "100%|██████████| 10000/10000 [01:01<00:00, 161.38it/s]\n"
     ]
    }
   ],
   "source": [
    "T = 1 # time horizon\n",
    "hidden_size = 100 # hidden layer size = 100\n",
    "lr = 0.01 # the learning rate for the optimizer\n",
    "num_epochs = 10000 # the number of iterations the optimizer will perform to train the neural network\n",
    "\n",
    "net = Net_DGM(2, hidden_size)\n",
    "# neural network is defined using the Net_DGM class with input dimension of 2 \n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "# initializes the Adam optimizer with the learning rate lr and the parameters of the neural network net. \n",
    "# The net.parameters() method returns an iterable of all the trainable parameters in the neural network net. \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# The mean squared error (MSE) loss function is defined using the nn.MSELoss() class.\n",
    "\n",
    "losses = [] # empty list prepared for value of the loss\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    #  t is a randomly selected value between 0 and T, which can be used as a time parameter for some stochastic process.\n",
    "    t = np.random.uniform(0, T)\n",
    "    # Two random numbers between -3 and 3 are generated and stored in the list x\n",
    "    x = [np.random.uniform(-3, 3), np.random.uniform(-3, 3)]\n",
    "    # An LQR object is created\n",
    "    lqr = LQR(H, M, sigma, C, R, D, 1)\n",
    "\n",
    "    # true value of the solution to the LQR problem\n",
    "    y_true = torch.tensor(lqr.getValue(torch.tensor([t]), torch.tensor([[x]], dtype=torch.float32)))\n",
    "    # optimizer's gradient is reset to zero\n",
    "    optimizer.zero_grad()\n",
    "    # predict the value of the solution to the LQR problem at time t and state x\n",
    "    y_pred = net(torch.tensor([[t]], dtype=torch.float32), torch.tensor([x]))\n",
    "\n",
    "    # The MSE loss between the predicted value and the true value\n",
    "    loss = criterion(y_pred, y_true)\n",
    "\n",
    "    # gradients of the neural network's parameters\n",
    "    loss.backward()\n",
    "    # optimizer takes a step in the negative direction of the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e8a5f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, '-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0a121",
   "metadata": {},
   "source": [
    "### Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18053401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\" \n",
    "    define a fully-connected feed-forward neural network.\n",
    "    sizes: a list of integers specifying the number of neurons in each layer of the neural network.\n",
    "    activation: an activation function to apply between hidden layers. By default, the ReLU activation function is used.\n",
    "    output_activation: an activation function to apply to the output layer. By default, the identity function is used.\n",
    "    batch_norm: a boolean flag indicating whether or not to apply batch normalization between layers. By default, this flag is set to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes, activation=nn.ReLU, output_activation=nn.Identity, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.BatchNorm1d(sizes[0]),] if batch_norm else []\n",
    "        for j in range(len(sizes)-1):\n",
    "            layers.append(nn.Linear(sizes[j], sizes[j+1]))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(sizes[j+1], affine=True))\n",
    "            if j<(len(sizes)-2):\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                layers.append(output_activation())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=True\n",
    "\n",
    "    \"\"\" \n",
    "    used to freeze and unfreeze the weights of the neural network, respectively. \n",
    "    When the weights are frozen, their gradients are not computed during backpropagation, effectively making them untrainable. \n",
    "    When the weights are unfrozen, their gradients are computed during backpropagation and the weights can be trained.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        # method is used to define the forward pass of the neural network. \n",
    "        # The output tensor of the last layer is returned as the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FFN([3, 100, 100, 2]) # A feedforward neural network is defined\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr) # Adam optimizer is defined with the learning rate specified\n",
    "criterion = nn.MSELoss() # A mean squared error loss function is defined.\n",
    "\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    t = np.random.uniform(0, T)\n",
    "    x = [np.random.uniform(-3, 3), np.random.uniform(-3, 3)]\n",
    "\n",
    "    # obtain the true value of the output at time t given the initial state x.\n",
    "    lqr = LQR(H, M, sigma, C, R, D, 1)\n",
    "    y_true = torch.tensor(lqr.getValue(torch.tensor([t]), torch.tensor([[x]], dtype=torch.float32)))\n",
    "\n",
    "    # reset the gradients in the optimizer to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Concatenate two tensors horizontally and pass through the neural network in a single operation\n",
    "    # ensure that the values of t and x are processed together by the neural network, rather than separately.\n",
    "    tx = torch.cat([torch.tensor([[t]], dtype=torch.float32), torch.tensor([x])], 1)\n",
    "    # use the neural network to predict the value of the solution to the LQR problem at time 't' and state 'x'\n",
    "    y_pred = net(tx)\n",
    "\n",
    "    # loss between the predicted value and the true value\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    # gradients of the neural network's parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model parameters to optimize the model parameters such that the loss function is minimized.\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5229aacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, '-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b2480",
   "metadata": {},
   "source": [
    "## Exercise 3 Deep Galerkin approximation for a linear PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee55d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(output, x):\n",
    "    \"\"\" \n",
    "    The function takes two arguments: output and x, which are both PyTorch tensors.\n",
    "    'output': This is the tensor that we want to compute the gradient of.\n",
    "    'x': This is the tensor with respect to which we want to compute the gradient.\n",
    "    \"\"\" \n",
    "    grad = torch.autograd.grad(output, x, grad_outputs=torch.ones_like(output), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    # returns the computed gradient tensor.\n",
    "    return grad\n",
    "\n",
    "def get_laplacian(grad, x):\n",
    "    \"\"\"\n",
    "    The function takes two arguments: grad and x, which are both PyTorch tensors.\n",
    "    'grad' represents the gradient tensor of some function with respect to x.\n",
    "    'x' represents the tensor with respect to which we want to compute the Laplacian of the function.\n",
    "    \"\"\"\n",
    "    hess = [] # empty list hess that will store the Hessian matrix for each dimension of x.\n",
    "    for d in range(x.shape[1]):\n",
    "        v = grad[:,d].view(-1,1) # This is a tensor that represents the gradient of a single component of grad with respect to x.\n",
    "        # calculate the Hessian matrices for each dimension of x, using torch.ones_like(v) to initialize the gradients to a tensor of ones with the same shape as the gradient tensor.\n",
    "        grad2 = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(v), only_inputs=True, create_graph=True, retain_graph=True)[0]\n",
    "        hess.append(grad2)\n",
    "    return torch.cat(hess, 1)\n",
    "\n",
    "class PDE_DGM(nn.Module):\n",
    "    \"\"\" \n",
    "    Defines a PyTorch module called PDE_DGM, which stands for \"partial differential equation deep Galerkin method\". \n",
    "    This module is used to solve partial differential equations (PDEs) using deep learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, hidden_dim: int, H, M, sigma, C, R, D, ts: torch.Tensor=None, device: str='cpu'):\n",
    "        # d: an integer representing the dimensionality of the PDE.\n",
    "        # hidden_dim: an integer representing the number of hidden units in the neural network.\n",
    "        # H, M, sigma, C, R, D: tensors representing the coefficients of the PDE.\n",
    "        # ts: a tensor representing the time points at which the PDE should be solved.\n",
    "        # device: a string representing the device (e.g., \"cpu\" or \"cuda\") on which to perform the computations.\n",
    "        super().__init__()\n",
    "        self.d = d \n",
    "        self.net_dgm = Net_DGM(d, hidden_dim, activation='Tanh')\n",
    "        self.H = torch.tensor(H, device=device)\n",
    "        self.M = torch.tensor(M, device=device)\n",
    "        self.sigma = torch.tensor(sigma, device=device)\n",
    "        self.C = torch.tensor(C, device=device)\n",
    "        self.R = torch.tensor(R, device=device)\n",
    "        self.D = torch.tensor(D, device=device)\n",
    "        self.alpha = torch.tensor([[1], [1]], device=device)\n",
    "        self.ts = ts\n",
    "\n",
    "    def fit(self, max_updates: int, batch_size: int, device):\n",
    "        \"\"\" \n",
    "        The fit method performs gradient descent on the neural network parameters \n",
    "        to minimize the mean squared error (MSE) between the predicted solution to the PDE and the actual solution.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.net_dgm.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = (10000,),gamma=0.1)\n",
    "        loss_fn = nn.MSELoss()\n",
    "    \n",
    "        pbar = tqdm(total=max_updates) # initializes a new progress bar with a maximum number of updates specified by max_updates. \n",
    "        \n",
    "        loss_value = []\n",
    "\n",
    "        # generates a random set of input points in the domain of the PDE, \n",
    "        # and evaluates the neural network at these points to obtain an approximate solution to the PDE.\n",
    "        for it in range(max_updates):\n",
    "            optimizer.zero_grad() # set all to zero\n",
    "            # generates a random tensor of shape (batch_size, self.d) with elements drawn uniformly from the range [-3, 3). \n",
    "            # ensures that the elements of the tensor are centered around zero\n",
    "            input_domain = 6 * torch.rand(batch_size, self.d, device=device, requires_grad=True) - 3\n",
    "            t0, T = self.ts[0], self.ts[-1] # time domain for the problem is defined by the range between t0 and T.\n",
    "            # generates a tensor of random numbers between 0 and 1 of shape (batch_size, 1) and assigns it to t. \n",
    "            t = t0 + T*torch.rand(batch_size, 1, device=device, requires_grad=True)\n",
    "\n",
    "            # t_T is a tensor of shape (batch_size, 1) containing the value of T for each batch element.\n",
    "            t_T = torch.ones(batch_size, 1, device=device) * T \n",
    "\n",
    "            # outputs of the neural network at t and T\n",
    "            u_of_tx = self.net_dgm(t, input_domain)\n",
    "            u_of_Tx = self.net_dgm(t_T, input_domain)\n",
    "\n",
    "            # the gradients of the neural network outputs with respect to the inputs 'input_domain' and 't'\n",
    "            grad_u_x = get_gradient(u_of_tx,input_domain)\n",
    "            grad_u_t = get_gradient(u_of_tx, t)\n",
    "\n",
    "            laplacian = get_laplacian(grad_u_x, input_domain)\n",
    "\n",
    "            # a target in the loss calculation for the PDE equation and boundary conditions\n",
    "            target = torch.zeros_like(u_of_tx)\n",
    "            R1, R2 = 0, 0\n",
    "            eqn = []\n",
    "            boundary = []\n",
    "\n",
    "            for i in range(laplacian.shape[0]):\n",
    "                x = laplacian[i]\n",
    "                temp_x = x.cpu().detach().numpy().reshape(2,2)\n",
    "                x_i = input_domain[i].cpu().detach().numpy()\n",
    "                grad_u_x_i = grad_u_x[i].cpu().detach().numpy()\n",
    "                alpha_np = self.alpha.cpu().numpy()\n",
    "                # for each sample in the batch, computes the temporary value for the PDE by evaluating the equation\n",
    "                temp_value = abs(grad_u_t[i].cpu().detach().numpy()\n",
    "                         + 1/2*np.trace(self.sigma.cpu().numpy() @ self.sigma.cpu().numpy().T @ temp_x)\n",
    "                         + grad_u_x_i.T @ self.H.cpu().numpy() @ x_i\n",
    "                         + grad_u_x_i.T @ self.M.cpu().numpy() @ alpha_np\n",
    "                         + grad_u_x_i.T @ self.M.cpu().numpy() @ alpha_np\n",
    "                         + x_i.T @ self.C.cpu().numpy() @ x_i\n",
    "                         + alpha_np.T @ self.D.cpu().numpy() @ alpha_np\n",
    "                        )**2\n",
    "                # appends this residual value to the list eqn.\n",
    "                eqn.append(temp_value.tolist()[0])\n",
    "\n",
    "                # computes the temporary value for the boundary condition by evaluating the equation:\n",
    "                temp_value = abs(u_of_Tx[i].cpu().detach().numpy() - x_i.T @ self.R.cpu().numpy() @ x_i)**2\n",
    "\n",
    "                # appends this residual value to the list boundary.\n",
    "                boundary.append(temp_value.tolist()[0])\n",
    "            \n",
    "            # lists are converted to PyTorch tensors\n",
    "            eqn = torch.tensor(eqn, device=device, requires_grad = True)\n",
    "            boundary = torch.tensor(boundary, device=device, requires_grad = True)\n",
    "            # loss values are calculated using the mean squared error (MSE) loss function\n",
    "            MSE_eqn = loss_fn(eqn, target) # the loss for the PDE equation\n",
    "            MSE_boundary = loss_fn(boundary, target) # the loss for the boundary conditions\n",
    "\n",
    "            loss = MSE_eqn + MSE_boundary # total loss\n",
    "            loss_value.append(loss.cpu().detach().item()) # stores the loss value in a list.\n",
    "            # computes the gradients of the loss with respect to all the learnable parameters in the neural network.\n",
    "            loss.backward()\n",
    "\n",
    "            # updates the weights of the network using the gradients computed in the previous step.\n",
    "            optimizer.step()\n",
    "            # updates the learning rate of the optimizer.\n",
    "            scheduler.step()\n",
    "            if it%10 == 0:\n",
    "                # update the progress bar (pbar) every 10 iterations and printing the current iteration number\n",
    "                pbar.update(10) \n",
    "                pbar.write(\"Iteration: {}/{}\\t MSE eqn: {:.4f}\\t MSE boundary: {:.4f}\\t Total Loss: {:.4f}\".format(it, max_updates, MSE_eqn.item(), MSE_boundary.item(), loss.item()))\n",
    "                \n",
    "        plt.plot(loss_value, '-')\n",
    "        plt.xlabel('Updates')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad37ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(T,\n",
    "        n_steps,\n",
    "        d,\n",
    "        hidden_dim,\n",
    "        max_updates,\n",
    "        batch_size, \n",
    "        base_dir,\n",
    "        device,\n",
    "        ):\n",
    "    \"\"\" \n",
    "    'T' is the maximum value of t\n",
    "    'n_steps' is the number of discrete points between 0 and T at which the function is sampled.\n",
    "    'd' is the dimension of the input space.\n",
    "    'hidden_dim' is the number of hidden units in the neural network.\n",
    "    'max_updates' is the maximum number of training steps to perform.\n",
    "    'batch_size' is the number of samples in each batch.\n",
    "    'base_dir' is the directory to save the trained model and log files.\n",
    "    'device' is the device on which to perform the computation (e.g., \"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "    # create a string that represents the path to the log file in the given directory\n",
    "    logfile = os.path.join(base_dir, \"log.txt\")\n",
    "\n",
    "    # create a tensor ts containing n_steps+1 equally spaced points between 0 and T.\n",
    "    ts = torch.linspace(0,T,n_steps+1, device=device)\n",
    "\n",
    "    # create 'PDE_DM' class\n",
    "    pde_solver = PDE_DGM(d, hidden_dim, H, M, sigma, C, R, D, ts=ts, device=device)\n",
    "    # move the model to the specified device \n",
    "    pde_solver.to(device)\n",
    "\n",
    "    # calls the fit method of the model to train it\n",
    "    pde_solver.fit(max_updates=max_updates,\n",
    "                     batch_size=batch_size, \n",
    "                     device=device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa2cbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\" \n",
    "    set the seed value for the random number generators\n",
    "    ensure that the same sequence of random numbers will be generated each time running the code\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # check if a GPU is available for use with PyTorch\n",
    "    device = \"cuda\" # device is set to \"cuda\"\n",
    "else:\n",
    "    device=\"cpu\" # if not, it is set to \"cpu\"\n",
    "\n",
    "# Sets a random seed\n",
    "seed = 1\n",
    "set_seed(seed)\n",
    "# Defines the path where the results of the training will be saved.\n",
    "results_path = os.path.join('./numerical_results/', \"PDE_DGM\", \"seed{}\".format(seed))\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "# Calls the train function\n",
    "train(T=1,\n",
    "    n_steps=50,\n",
    "    d=2,\n",
    "    hidden_dim=100,\n",
    "    max_updates=5000,\n",
    "    batch_size=500,\n",
    "    base_dir=results_path,\n",
    "    device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8c819",
   "metadata": {},
   "source": [
    "## Exercise 4 Policy iteration with DGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794adfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class DGM_layer(nn.Module):\n",
    "    \"\"\" \n",
    "    Define a single DGM layer in the network. It inherits from the nn.Module class in PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_feature, residual = False):\n",
    "        super(DGM_layer, self).__init__()\n",
    "        self.residual = residual # a boolean indicating whether or not to use residual connections in the layer.\n",
    "        \n",
    "        # The layer contains four linear transformation operations, each transformation consists of a linear layer\n",
    "        self.Z = nn.Linear(out_feature, out_feature); self.UZ = nn.Linear(in_features, out_feature, bias = False)\n",
    "        self.G = nn.Linear(out_feature, out_feature); self.UG = nn.Linear(in_features, out_feature, bias = False)\n",
    "        self.R = nn.Linear(out_feature, out_feature); self.UR = nn.Linear(in_features, out_feature, bias = False)\n",
    "        self.H = nn.Linear(out_feature, out_feature); self.UH = nn.Linear(in_features, out_feature, bias = False)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        \"\"\" \n",
    "        takes the input x and the previous state s as inputs\n",
    "        returns the new state after applying the DGM layer transformations.\n",
    "        the tanh activation function is used to transform the input x and the state s through several linear layers\n",
    "        \"\"\"\n",
    "        #  x and s are passed through linear layers to generate four intermediate tensors \n",
    "        z = torch.tanh(self.UZ(x)+self.Z(s))\n",
    "        g = torch.tanh(self.UG(x)+self.G(s))\n",
    "        r = torch.tanh(self.UR(x)+self.R(s))\n",
    "        h = torch.tanh(self.UH(x)+self.H(s))\n",
    "        return (1-g)*h+z*s  \n",
    "    \n",
    "class DGM_Net(nn.Module):\n",
    "    \"\"\" \n",
    "    The DGM_Net class is defined as a subclass of nn.Module, which is a PyTorch class for defining neural network modules.\n",
    "    in_dim: the number of input features\n",
    "    out_dim: the number of output features\n",
    "    n_layers: the number of layers in the DGM\n",
    "    n_neurons: the number of neurons in each layer of the DGM\n",
    "    residual: a Boolean value indicating whether to use residual connections in the DGM\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, n_layers, n_neurons, residual = False): \n",
    "        super(DGM_Net, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.residual = residual\n",
    "        \n",
    "        # a linear layer that maps the input to the number of neurons in the first layer of the DGM\n",
    "        self.input_layer = nn.Linear(in_dim, n_neurons) \n",
    "        # a list of n_layers DGM_layer objects that implement the DGM layer defined earlier\n",
    "        self.dgm_layers = nn.ModuleList([DGM_layer(self.in_dim, self.n_neurons, self.residual) for i in range(self.n_layers)])\n",
    "        # a linear layer that maps the output of the last layer of the DGM to the number of output features\n",
    "        self.output_layer = nn.Linear(n_neurons, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = torch.tanh(self.input_layer(x))\n",
    "        # iterate over the DGM layers and apply each layer to the output of the previous layer.\n",
    "        for i, dgm_layer in enumerate(self.dgm_layers):\n",
    "            s = dgm_layer(x, s)\n",
    "\n",
    "            # obtain the final output of the network\n",
    "        return self.output_layer(s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e8eeedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bellman_pde():\n",
    "    '''\n",
    "    Approximating the Bellman PDE on [0,T]*[x1_l,x1_r]*[x2_l,x2_r]\n",
    "    '''\n",
    "    def __init__(self, net, x_interval, y_interval, H, M, C, D, R, T, sigma, a):\n",
    "        self.net = net \n",
    "        self.x1_l = x_interval[0].item() # torch tensor, dim = 3\n",
    "        self.x1_r = x_interval[1].item()\n",
    "        self.x2_l = y_interval[0].item()\n",
    "        self.x2_r = y_interval[1].item()\n",
    "        self.H = H # H, M, C, D, R: torch tensors, dim = 2*2\n",
    "        self.M = M \n",
    "        self.C = C \n",
    "        self.D = D \n",
    "        self.R = R         \n",
    "        self.T = T # integer\n",
    "        self.sigma = sigma # sigma, a: torch tensors, dim = 1*2\n",
    "        self.a = a \n",
    "        \n",
    "    def sample(self, size):\n",
    "        # this method is used to sample size number of points from the domain of the PDE. \n",
    "        t_x = torch.cat((torch.rand([size, 1])*self.T, (self.x1_l - self.x1_r) * torch.rand([size, 1]) + self.x1_r, (self.x2_l - self.x2_r) * torch.rand([size, 1]) + self.x2_r), dim=1)\n",
    "        x_boundary = torch.cat((torch.ones(size, 1)*self.T, (self.x1_l - self.x1_r) * torch.rand([size, 1]) + self.x1_r, (self.x2_l - self.x2_r) * torch.rand([size, 1]) + self.x2_r), dim=1)     \n",
    "        return t_x, x_boundary\n",
    "    \n",
    "    def mat_ext(self, mat, size):\n",
    "        # this method used to extend a given matrix to a 3-dimensional tensor of a certain size.\n",
    "        if mat.shape == torch.Size([2, 2]):\n",
    "            # returns a 3-dimensional tensor of size \"(size, 2, 2)\"\n",
    "            return mat.unsqueeze(0).repeat(size,1,1)\n",
    "        elif mat.shape == torch.Size([1, 2]):\n",
    "            # create a 3-dimensional tensor of size (size, 1, 2) \n",
    "            return mat.t().unsqueeze(0).repeat(size,1,1)\n",
    "        \n",
    "    def get_hessian(self, grad_x, x):\n",
    "        \"\"\" \n",
    "        This method calculates the Hessian matrix of a function with respect to its input variables at a given input point x. \n",
    "        \"\"\"\n",
    "        hessian = torch.zeros(len(x),2,2)\n",
    "        dxx = torch.autograd.grad(grad_x[0][:,1], x, grad_outputs=torch.ones_like(grad_x[0][:,1]), allow_unused=True, retain_graph=True)[0][:,1]\n",
    "        dxy = torch.autograd.grad(grad_x[0][:,1], x, grad_outputs=torch.ones_like(grad_x[0][:,1]), allow_unused=True, retain_graph=True)[0][:,2]\n",
    "        dyx = torch.autograd.grad(grad_x[0][:,2], x, grad_outputs=torch.ones_like(grad_x[0][:,2]), allow_unused=True, retain_graph=True)[0][:,1]\n",
    "        dyy = torch.autograd.grad(grad_x[0][:,2], x, grad_outputs=torch.ones_like(grad_x[0][:,2]), allow_unused=True, retain_graph=True)[0][:,2]\n",
    "        hessian[:,0,0] = dxx \n",
    "        hessian[:,0,1] = dxy\n",
    "        hessian[:,1,0] = dyx\n",
    "        hessian[:,1,1] = dyy\n",
    "        return hessian  \n",
    "        \n",
    "        \n",
    "    def loss_func(self, size):\n",
    "        loss = nn.MSELoss() # MSE \n",
    "        \n",
    "        # Extend the input matrices\n",
    "        H = self.mat_ext(self.H, size) # H, M, C, D, R: dim = batchsize*2*2\n",
    "        M = self.mat_ext(self.M, size)\n",
    "        C = self.mat_ext(self.C, size)\n",
    "        D = self.mat_ext(self.D, size)\n",
    "        R = self.mat_ext(self.R, size) # control: dim = batchsize*2*1          \n",
    "        T = self.T\n",
    "        a = self.a\n",
    "        sig = self.sigma.t()\n",
    "\n",
    "        x, x_boundary = self.sample(size=size)\n",
    "        x = x.requires_grad_(True) # Track gradients during automatic differentiation\n",
    "\n",
    "        # gradients\n",
    "        grad = torch.autograd.grad(self.net(x), x, grad_outputs=torch.ones_like(self.net(x)), create_graph=True)\n",
    "        \n",
    "        du_dt = grad[0][:,0].reshape(-1, 1)  # derivative w.r.t. time, dim = batchsize*1\n",
    "        \n",
    "        du_dx = grad[0][:,1:] # derivative w.r.t. space, dim = batchsize*2 \n",
    "                \n",
    "        # Hessian matrix\n",
    "        hessian = self.get_hessian(grad,x)\n",
    "        \n",
    "        # Error from the equation\n",
    "        sig2_ext = self.mat_ext(torch.matmul(sig,sig.t()), size) # dim = batchsize*2*2\n",
    "        prod = torch.bmm(sig2_ext,hessian) # sigma*sigma^T*2nd derivatives\n",
    "        trace = torch.diagonal(prod, dim1=1, dim2=2).sum(dim=1).unsqueeze(0).t() # trace, dim = batchsize*1\n",
    "        x_space = x[:,1:].unsqueeze(1).reshape(size,2,1) # extract (x1,x2)^T, dim = batchsize*2*1\n",
    "        x_space_t = x_space.reshape(size,1,2) # dim = batchsize*1*2\n",
    "        du_dx_ext_t = du_dx.unsqueeze(1) # dim=batchsize*1*2\n",
    "        \n",
    "        pde = du_dt+0.5*trace+torch.bmm(du_dx_ext_t,torch.bmm(H,x_space)).squeeze(1)\\\n",
    "                +torch.bmm(du_dx_ext_t,torch.bmm(M,self.a)).squeeze(1)\\\n",
    "                +torch.bmm(x_space_t,torch.bmm(C,x_space)).squeeze(1)\\\n",
    "                +torch.bmm(a.reshape(size,1,2),torch.bmm(D,a)).squeeze(1) # dim = batchsize*1\n",
    " \n",
    "        pde_err = loss(pde, torch.zeros(size,1))\n",
    "        \n",
    "        # Error from the boundary condition\n",
    "        x_bound = x_boundary[:,1:].unsqueeze(1).reshape(size,2,1) # extract (x1,x2)^T, dim = batchsize*2*1\n",
    "        x_bound_t = x_bound.reshape(size,1,2) # dim = batchsize*1*2\n",
    "        \n",
    "        boundary_err = loss(self.net(x_boundary), torch.bmm(x_bound_t,torch.bmm(R,x_bound)).squeeze(1))\n",
    "        \n",
    "        return pde_err + boundary_err\n",
    "\n",
    "\n",
    "class Train():\n",
    "    \"\"\" \n",
    "    This is a class called Train that is used to train a neural network (net) to solve a PDE\n",
    "    \"\"\"\n",
    "    def __init__(self, net, PDE, BATCH_SIZE):\n",
    "        self.errors = []\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.net = net # neural network\n",
    "        self.model = PDE \n",
    "\n",
    "    def train(self, epoch, lr):\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr) # Import the parameters, lr: learning rate\n",
    "        avg_loss = 0\n",
    "        for e in range(epoch):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.model.loss_func(self.BATCH_SIZE) # calculate the loss using the loss function defined in the PDE class\n",
    "            avg_loss = avg_loss + float(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step() # updates the optimizer weights\n",
    "            if (e+1) % 100 == 0:\n",
    "                loss = avg_loss/100\n",
    "                # print the current epoch, learning rate, and loss every 100 epochs.\n",
    "                print(\"epoch {} - lr {} - loss: {}\".format(e, lr, loss))\n",
    "                avg_loss = 0\n",
    "\n",
    "                error = self.model.loss_func(self.BATCH_SIZE)\n",
    "                self.errors.append(error.detach())\n",
    "\n",
    "    def get_errors(self):\n",
    "        #  return a list of errors that were generated during the training process.\n",
    "        return self.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "272f035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LQR:\n",
    "    def __init__(self, H, M, D, C, R, sigma, T):\n",
    "        self.H = H.double()\n",
    "        self.M = M.double()\n",
    "        self.D = D.double()\n",
    "        self.C = C.double()\n",
    "        self.R = R.double()\n",
    "        self.sigma = sigma.reshape(1,-1).t() # reshape the input sigma as a 2*1 matrix\n",
    "        self.T = T\n",
    "\n",
    "\n",
    "    def riccati_ode(self, t, Q):\n",
    "        if type(self.C) == torch.Tensor:\n",
    "             self.C = self.C.numpy()\n",
    "\n",
    "        # Rewrite the imput 1*4 vector as a 2*2 matrix\n",
    "    \n",
    "        Q_matrix = Q.reshape((2,2))\n",
    "        \n",
    "        # RHS of the ode\n",
    "        quadratic_term = -np.linalg.multi_dot([Q_matrix,self.M,np.linalg.inv(self.D),self.M,Q_matrix])\n",
    "        linear_term = 2*np.dot(np.transpose(self.H),Q_matrix)\n",
    "        constant_term = self.C\n",
    "        \n",
    "        # Riccati ode in the matrix form\n",
    "        dQ_dt_matrix = linear_term + quadratic_term + constant_term\n",
    "        \n",
    "        # Rewrite the matrix ode as a 1*4 vector\n",
    "        dQ_dt = dQ_dt_matrix.reshape(4,)\n",
    "\n",
    "        # return the time derivative of the state vector Q in the Riccati differential equation\n",
    "        return dQ_dt\n",
    "\n",
    "    \n",
    "    def riccati_solver(self, time_grid):\n",
    "        if type(time_grid) == torch.Tensor:\n",
    "            time_grid = time_grid.numpy()\n",
    "\n",
    "        Q_0 = self.R.reshape(4,) # initial condition: Q(0)=S(T)=R\n",
    "\n",
    "        # Solving S(r) on [t,T] is equivalent to solving Q(r)=S(T-r) on [0,T-t] \n",
    "        time_grid_Q = np.flip(self.T-time_grid) \n",
    "        interval = np.array([time_grid_Q[0], time_grid_Q[-1]]) \n",
    "        sol = integrate.solve_ivp(self.riccati_ode, interval, Q_0, t_eval=time_grid_Q)\n",
    "\n",
    "        t_val = self.T - sol.t # do the time-reversal to get the solution S(t)\n",
    "\n",
    "        return np.flip(t_val), np.flip(sol.y) # returns the solution of the Riccati equation for the given time grid. \n",
    "        \n",
    "    def riccati_plot(self, time_grid):\n",
    "        sol_t, sol_y = self.riccati_solver(time_grid)\n",
    "        plt.plot(sol_t,sol_y[0],label='S[0,0]',color='blue')\n",
    "        plt.plot(sol_t,sol_y[1],label='S[0,1]',color='red')\n",
    "        plt.plot(sol_t,sol_y[2],label='S[1,0]',color='yellow')\n",
    "        plt.plot(sol_t,sol_y[3],label='S[1,1]',color='purple')\n",
    "\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('S(t)')\n",
    "        plt.legend(['S[0,0]','S[0,1]','S[1,0]','S[1,1]'])\n",
    "        plt.show()\n",
    "\n",
    "    def value_function(self, t, x):\n",
    "        \"\"\" \n",
    "        compute the value function V(t,x) for a given time t and asset values x.\n",
    "        returns a 2-dimensional tensor of shape (len(x), 1), containing the computed value function for each input asset value.\n",
    "        \"\"\"\n",
    "        n = 500 # Fix the number of steps to be 500\n",
    "        val_func = torch.zeros((len(x),1), dtype=torch.float64) \n",
    "        x = x.double()\n",
    "\n",
    "        for j in range(len(x)):\n",
    "            initial_time = t[j].double().item() \n",
    "            step = (self.T-initial_time)/n # step = (T-t)/n\n",
    "            time_grid = torch.arange(initial_time, self.T+step, step) # generate the time grid on [t,T]\n",
    "            t_val, S_r = self.riccati_solver(time_grid)   \n",
    "            S_t = torch.tensor([[S_r[0,0], S_r[1,0]], [S_r[2,0], S_r[3,0]]]) \n",
    "            S_t = S_t.double() # solution of the Riccati ODE\n",
    "\n",
    "            # Assuming sigma is 2x1\n",
    "            sig = torch.matmul(self.sigma, self.sigma.t()) # 2*2 matrix\n",
    "            sig = sig.double()\n",
    "            integral = 0\n",
    "            for i in range(len(t_val)-1):\n",
    "                S_i = torch.tensor([[S_r[0,i], S_r[1,i]], [S_r[2,i], S_r[3,i]]])\n",
    "                S_i_1 = torch.tensor([[S_r[0,i+1], S_r[1,i+1]], [S_r[2,i+1], S_r[3,i+1]]])\n",
    "                difference = S_i_1-S_i\n",
    "                integral += torch.trace(torch.matmul(sig,difference))*(t_val[i+1] - t_val[i])\n",
    "\n",
    "            x_j = x[j].reshape(1,-1).t()\n",
    "            x_j_t = x_j.t()\n",
    "\n",
    "            # computes the value function by multiplying the asset values x with the S_t, and adding the integral term.\n",
    "            val_func[j] = torch.linalg.multi_dot([x_j_t,S_t,x_j]) + integral\n",
    "\n",
    "        return val_func\n",
    "        \n",
    "\n",
    "    def optimal_control(self, t, x):\n",
    "        \"\"\" \n",
    "        calculate the optimal control for the stochastic linear quadratic control problem.\n",
    "        It takes in the current time 't' and state 'x' and calculates the optimal control 'a' for the given state using the Riccati equation solution S_t. \n",
    "        \"\"\"\n",
    "        n = 500\n",
    "        a = torch.zeros(len(x), 2)\n",
    "        x = x.double()\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            init_time = t[i].double().item() \n",
    "            step = (self.T-init_time)/n # step = (T-t)/n\n",
    "            time_grid = torch.arange(init_time, self.T+step, step) # generate the time grid on [t,T]\n",
    "            S_r = self.riccati_solver(time_grid)[1]\n",
    "            S_t = torch.tensor([[S_r[0,0], S_r[1,0]], [S_r[2,0], S_r[3,0]]]) \n",
    "            S_t = S_t.double()\n",
    "            x_i = x[i].reshape(1,-1).t() \n",
    "\n",
    "            # The product is 2*1, need to flatten it first before appending the value to a_star\n",
    "            a[i] = -torch.flatten(torch.linalg.multi_dot([self.D,self.M.t(),S_t,x_i])) \n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49bd7b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0874, -0.1522],\n",
      "        [ 0.0523, -0.2180]], grad_fn=<AddmmBackward0>)\n",
      "torch.FloatTensor\n",
      "epoch 99 - lr 0.001 - loss: 17.29264768898487\n",
      "epoch 199 - lr 0.001 - loss: 5.8908077363669875\n",
      "epoch 299 - lr 0.001 - loss: 3.162780873477459\n",
      "epoch 399 - lr 0.001 - loss: 2.359874303638935\n",
      "epoch 499 - lr 0.001 - loss: 1.2696894023567438\n",
      "epoch 599 - lr 0.001 - loss: 1.548354350924492\n",
      "epoch 699 - lr 0.001 - loss: 1.3476471900939941\n",
      "epoch 799 - lr 0.001 - loss: 0.9657802438735962\n",
      "epoch 899 - lr 0.001 - loss: 1.2405791057646274\n",
      "epoch 999 - lr 0.001 - loss: 1.404521966110915\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 2\n",
    "\n",
    "# Set parameters\n",
    "H = torch.tensor([[1.0,0],[0,1.0]])\n",
    "M = torch.tensor([[1.0,0],[0,1.0]])\n",
    "D = torch.tensor([[0.1,0],[0,0.1]])\n",
    "C = torch.tensor([[0.1,0],[0,0.1]])\n",
    "R = torch.tensor([[1.0,0],[0,1.0]])\n",
    "sigma = torch.tensor([[0.05, 0.05]])\n",
    "\n",
    "# Create data t and x\n",
    "T = 1.0\n",
    "x_range = torch.tensor([-3, 3])\n",
    "y_range = torch.tensor([-3, 3])\n",
    "t = np.random.uniform(0, T, size=batch_size)\n",
    "x = np.random.uniform(-3, 3, size=(batch_size, 1, 2))\n",
    "t0 = torch.from_numpy(np.array([t]).T).float()\n",
    "x0 = torch.from_numpy(x.reshape(batch_size, 2)).float()\n",
    "tx = torch.cat([t0,x0], dim=1)\n",
    "\n",
    "# Convert numpy to torch tensor\n",
    "t = torch.from_numpy(t)\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "# Determine the value function for the samples of t and x\n",
    "lqr_equation = LQR(H, M, D, C, R, sigma, T)\n",
    "# calculate the optimal control \n",
    "opt_control = lqr_equation.optimal_control(t ,x).float()\n",
    "# calculate the value function\n",
    "value_func = lqr_equation.value_function(t, x,).float()\n",
    " \n",
    "# Input for FFN neural network (control function)\n",
    "dim = [3,100,100,2] \n",
    "# Input for Net_DGM  neural network (value function)\n",
    "value_dim_input = 2 \n",
    "value_dim_hidden = 100\n",
    "\n",
    "# Input for DGM_Net neural network (PDE)\n",
    "dim_input = 3\n",
    "dim_output = 1\n",
    "num_layers = 3\n",
    "num_neurons = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the control model, loss function, and Adam optimizer\n",
    "control_model = FFN(sizes=dim)\n",
    "# control_loss_fn = nn.MSELoss()\n",
    "control_optimizer = optim.Adam(control_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initailize the value function model, loss function, and Adam optimizer\n",
    "value_model = Net_DGM(value_dim_input, value_dim_hidden)\n",
    "value_optimizer = optim.Adam(value_model.parameters(), lr=learning_rate)\n",
    "\n",
    "alpha_pred = control_model(tx) # predicted optimal control for the given input tx\n",
    "print(alpha_pred)\n",
    "\n",
    "# reshape the control to (batch_size, 2, 1), which is the required shape for the alpha argument of the Bellman_pde \n",
    "alpha_pred = alpha_pred.unsqueeze(1).reshape(batch_size,2,1).clone().detach()\n",
    "\n",
    "print(alpha_pred.type())\n",
    "\n",
    "net = DGM_Net(dim_input, dim_output, num_layers, num_neurons) # initialize the neural network\n",
    "Bellman = Bellman_pde(net, x_range, y_range, H, M, C, D, R, T, sigma, alpha_pred)\n",
    "train = Train(net, Bellman, BATCH_SIZE=batch_size)\n",
    "train.train(epoch=n_epochs, lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f695691c7675195da2229e392149ef068beb920cff0b2fc3ea04696403466e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
