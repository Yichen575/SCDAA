{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8368fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class DGM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(DGM_Layer, self).__init__()\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "            \n",
    "\n",
    "        self.gate_Z = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_G = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_R = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_H = self.layer(dim_x+dim_S, dim_S)\n",
    "            \n",
    "    def layer(self, nIn, nOut):\n",
    "        l = nn.Sequential(nn.Linear(nIn, nOut), self.activation)\n",
    "        return l\n",
    "    \n",
    "    def forward(self, x, S):\n",
    "        x_S = torch.cat([x,S],1)\n",
    "        Z = self.gate_Z(x_S)\n",
    "        G = self.gate_G(x_S)\n",
    "        R = self.gate_R(x_S)\n",
    "        \n",
    "        input_gate_H = torch.cat([x, S*R],1)\n",
    "        H = self.gate_H(input_gate_H)\n",
    "        \n",
    "        output = ((1-G))*H + Z*S\n",
    "        return output\n",
    "\n",
    "\n",
    "class Net_DGM(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(Net_DGM, self).__init__()\n",
    "\n",
    "        self.dim = dim_x\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(dim_x+1, dim_S), self.activation)\n",
    "\n",
    "        self.DGM1 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM2 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM3 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "\n",
    "        self.output_layer = nn.Linear(dim_S, 1)\n",
    "\n",
    "    def forward(self,t,x):\n",
    "        tx = torch.cat([t,x], 1)\n",
    "        S1 = self.input_layer(tx)\n",
    "        S2 = self.DGM1(tx,S1)\n",
    "        S3 = self.DGM2(tx,S2)\n",
    "        S4 = self.DGM3(tx,S3)\n",
    "        output = self.output_layer(S4)\n",
    "        return output\n",
    "\n",
    "class DGM_layer(nn.Module):\n",
    "    def __init__(self, in_features, out_feature, residual = False):\n",
    "        super(DGM_layer, self).__init__()\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.Z = nn.Linear(out_feature, out_feature); self.UZ = nn.Linear(in_features, out_feature, bias = False)\n",
    "        self.G = nn.Linear(out_feature, out_feature); self.UG = nn.Linear(in_features, out_feature, bias = False)\n",
    "        self.R = nn.Linear(out_feature, out_feature); self.UR = nn.Linear(in_features, out_feature, bias = False)\n",
    "        self.H = nn.Linear(out_feature, out_feature); self.UH = nn.Linear(in_features, out_feature, bias = False)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        z = torch.tanh(self.UZ(x)+self.Z(s))\n",
    "        g = torch.tanh(self.UG(x)+self.G(s))\n",
    "        r = torch.tanh(self.UR(x)+self.R(s))\n",
    "        h = torch.tanh(self.UH(x)+self.H(s))\n",
    "        return (1-g)*h+z*s  \n",
    "    \n",
    "class DGM_Net(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_layers, n_neurons, residual = False): \n",
    "        super(DGM_Net, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.residual = residual\n",
    "    \n",
    "        self.input_layer = nn.Linear(in_dim, n_neurons)\n",
    "        self.dgm_layers = nn.ModuleList([DGM_layer(self.in_dim, self.n_neurons, self.residual) for i in range(self.n_layers)])\n",
    "        self.output_layer = nn.Linear(n_neurons, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = torch.tanh(self.input_layer(x))\n",
    "        for i, dgm_layer in enumerate(self.dgm_layers):\n",
    "            s = dgm_layer(x, s)\n",
    "        return self.output_layer(s)\n",
    "    \n",
    "def get_gradient(output, x):\n",
    "    grad = torch.autograd.grad(output, x, grad_outputs=torch.ones_like(output), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    return grad\n",
    "\n",
    "def get_laplacian(grad, x):\n",
    "    hess_diag = []\n",
    "    for d in range(x.shape[1]):\n",
    "        v = grad[:,d].view(-1,1)\n",
    "        grad2 = torch.autograd.grad(v,x,grad_outputs=torch.ones_like(v), only_inputs=True, create_graph=True, retain_graph=True)[0]\n",
    "        hess_diag.append(grad2[:,d].view(-1,1))    \n",
    "    hess_diag = torch.cat(hess_diag,1)\n",
    "    laplacian = hess_diag.sum(1, keepdim=True)\n",
    "    return laplacian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99539df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bellman_pde():\n",
    "    '''\n",
    "    Approximating the Bellman PDE on [0,T]*[x1_l,x1_r]*[x2_l,x2_r]\n",
    "    '''\n",
    "    def __init__(self, net, x_interval, y_interval, H, M, C, D, R, T, sigma, a):\n",
    "        self.net = net \n",
    "        self.x1_l = x_interval[0].item() # torch tensor, dim = 3\n",
    "        self.x1_r = x_interval[1].item()\n",
    "        self.x2_l = y_interval[0].item()\n",
    "        self.x2_r = y_interval[1].item()\n",
    "        self.H = H # H, M, C, D, R: torch tensors, dim = 2*2\n",
    "        self.M = M \n",
    "        self.C = C \n",
    "        self.D = D \n",
    "        self.R = R         \n",
    "        self.T = T # integer\n",
    "        self.sigma = sigma # sigma, a: torch tensors, dim = 1*2\n",
    "        self.a = a \n",
    "        \n",
    "    def sample(self, size):\n",
    "    \n",
    "        t_x = torch.cat((torch.rand([size, 1])*self.T, (self.x1_l - self.x1_r) * torch.rand([size, 1]) + self.x1_r, (self.x2_l - self.x2_r) * torch.rand([size, 1]) + self.x2_r), dim=1)\n",
    "        \n",
    "        x_boundary = torch.cat((torch.ones(size, 1)*self.T, (self.x1_l - self.x1_r) * torch.rand([size, 1]) + self.x1_r, (self.x2_l - self.x2_r) * torch.rand([size, 1]) + self.x2_r), dim=1)     \n",
    "        return t_x, x_boundary\n",
    "    \n",
    "    def mat_ext(self, mat, size):\n",
    "        if mat.shape == torch.Size([2, 2]):\n",
    "            return mat.unsqueeze(0).repeat(size,1,1)\n",
    "        elif mat.shape == torch.Size([1, 2]):\n",
    "            return mat.t().unsqueeze(0).repeat(size,1,1)\n",
    "        \n",
    "    def get_hessian(self, grad_x, x):\n",
    "        hessian = torch.zeros(len(x),2,2)\n",
    "        dxx = torch.autograd.grad(grad_x[0][:,1], x, grad_outputs=torch.ones_like(grad_x[0][:,1]), allow_unused=True, retain_graph=True)[0][:,1]\n",
    "        dxy = torch.autograd.grad(grad_x[0][:,1], x, grad_outputs=torch.ones_like(grad_x[0][:,1]), allow_unused=True, retain_graph=True)[0][:,2]\n",
    "        dyx = torch.autograd.grad(grad_x[0][:,2], x, grad_outputs=torch.ones_like(grad_x[0][:,2]), allow_unused=True, retain_graph=True)[0][:,1]\n",
    "        dyy = torch.autograd.grad(grad_x[0][:,2], x, grad_outputs=torch.ones_like(grad_x[0][:,2]), allow_unused=True, retain_graph=True)[0][:,2]\n",
    "        hessian[:,0,0] = dxx \n",
    "        hessian[:,0,1] = dxy\n",
    "        hessian[:,1,0] = dyx\n",
    "        hessian[:,1,1] = dyy\n",
    "        return hessian  \n",
    "        \n",
    "        \n",
    "    def loss_func(self, size):\n",
    "        loss = nn.MSELoss() # MSE \n",
    "        \n",
    "        # Extend the input matrices\n",
    "        H = self.mat_ext(self.H, size) # H, M, C, D, R: dim = batchsize*2*2\n",
    "        M = self.mat_ext(self.M, size)\n",
    "        C = self.mat_ext(self.C, size)\n",
    "        D = self.mat_ext(self.D, size)\n",
    "        R = self.mat_ext(self.R, size) # control: dim = batchsize*2*1          \n",
    "        T = self.T\n",
    "        a = self.a\n",
    "        sig = self.sigma.t()\n",
    "\n",
    "        x, x_boundary = self.sample(size=size)\n",
    "        x = x.requires_grad_(True) # Track gradients during automatic differentiation\n",
    "\n",
    "        # gradients\n",
    "        grad = torch.autograd.grad(self.net(x), x, grad_outputs=torch.ones_like(self.net(x)), create_graph=True)\n",
    "        \n",
    "        du_dt = grad[0][:,0].reshape(-1, 1)  # derivative w.r.t. time, dim = batchsize*1\n",
    "        \n",
    "        du_dx = grad[0][:,1:] # derivative w.r.t. space, dim = batchsize*2 \n",
    "                \n",
    "        # Hessian matrix\n",
    "        hessian = self.get_hessian(grad,x)\n",
    "        \n",
    "        # Error from the equation\n",
    "        sig2_ext = self.mat_ext(torch.matmul(sig,sig.t()), size) # dim = batchsize*2*2\n",
    "        prod = torch.bmm(sig2_ext,hessian) # sigma*sigma^T*2nd derivatives\n",
    "        trace = torch.diagonal(prod, dim1=1, dim2=2).sum(dim=1).unsqueeze(0).t() # trace, dim = batchsize*1\n",
    "        x_space = x[:,1:].unsqueeze(1).reshape(size,2,1) # extract (x1,x2)^T, dim = batchsize*2*1\n",
    "        x_space_t = x_space.reshape(size,1,2) # dim = batchsize*1*2\n",
    "        du_dx_ext_t = du_dx.unsqueeze(1) # dim=batchsize*1*2\n",
    "        \n",
    "        pde = du_dt+0.5*trace+torch.bmm(du_dx_ext_t,torch.bmm(H,x_space)).squeeze(1)\\\n",
    "                +torch.bmm(du_dx_ext_t,torch.bmm(M,self.a)).squeeze(1)\\\n",
    "                +torch.bmm(x_space_t,torch.bmm(C,x_space)).squeeze(1)\\\n",
    "                +torch.bmm(a.reshape(size,1,2),torch.bmm(D,a)).squeeze(1) # dim = batchsize*1\n",
    " \n",
    "        pde_err = loss(pde, torch.zeros(size,1))\n",
    "        \n",
    "        # Error from the boundary condition\n",
    "        x_bound = x_boundary[:,1:].unsqueeze(1).reshape(size,2,1) # extract (x1,x2)^T, dim = batchsize*2*1\n",
    "        x_bound_t = x_bound.reshape(size,1,2) # dim = batchsize*1*2\n",
    "        \n",
    "        boundary_err = loss(self.net(x_boundary), torch.bmm(x_bound_t,torch.bmm(R,x_bound)).squeeze(1))\n",
    "        \n",
    "        return pde_err + boundary_err\n",
    "\n",
    "\n",
    "class Train():\n",
    "    def __init__(self, net, PDE, BATCH_SIZE):\n",
    "        self.errors = []\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.net = net\n",
    "        self.model = PDE\n",
    "\n",
    "    def train(self, epoch, lr):\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr) # Import the parameters, lr: learning rate\n",
    "        avg_loss = 0\n",
    "        for e in range(epoch):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.model.loss_func(self.BATCH_SIZE)\n",
    "            avg_loss = avg_loss + float(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (e+1) % 100 == 0:\n",
    "                loss = avg_loss/100\n",
    "                print(\"epoch {} - lr {} - loss: {}\".format(e, lr, loss))\n",
    "                avg_loss = 0\n",
    "\n",
    "                error = self.model.loss_func(self.BATCH_SIZE)\n",
    "                self.errors.append(error.detach())\n",
    "\n",
    "    def get_errors(self):\n",
    "        return self.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee500c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "\n",
    "    def __init__(self, sizes, activation=nn.ReLU, output_activation=nn.Identity, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.BatchNorm1d(sizes[0]),] if batch_norm else []\n",
    "        for j in range(len(sizes)-1):\n",
    "            layers.append(nn.Linear(sizes[j], sizes[j+1]))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(sizes[j+1], affine=True))\n",
    "            if j<(len(sizes)-2):\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                layers.append(output_activation())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def freeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3514013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LQR:\n",
    "    def __init__(self, H, M, D, C, R, sigma, T):\n",
    "        self.H = H.double()\n",
    "        self.M = M.double()\n",
    "        self.D = D.double()\n",
    "        self.C = C.double()\n",
    "        self.R = R.double()\n",
    "        self.sigma = sigma.reshape(1,-1).t() # reshape the input sigma as a 2*1 matrix\n",
    "        self.T = T\n",
    "\n",
    "\n",
    "    def riccati_ode(self, t, Q):\n",
    "        if type(self.C) == torch.Tensor:\n",
    "             self.C = self.C.numpy()\n",
    "\n",
    "        # Rewrite the imput 1*4 vector as a 2*2 matrix\n",
    "    \n",
    "        Q_matrix = Q.reshape((2,2))\n",
    "        \n",
    "        # RHS of the ode\n",
    "        quadratic_term = -np.linalg.multi_dot([Q_matrix,self.M,np.linalg.inv(self.D),self.M,Q_matrix])\n",
    "        linear_term = 2*np.dot(np.transpose(self.H),Q_matrix)\n",
    "        constant_term = self.C\n",
    "        \n",
    "        # Riccati ode in the matrix form\n",
    "        dQ_dt_matrix = linear_term + quadratic_term + constant_term\n",
    "        \n",
    "        # Rewrite the matrix ode as a 1*4 vector\n",
    "        dQ_dt = dQ_dt_matrix.reshape(4,)\n",
    "        \n",
    "        return dQ_dt\n",
    "\n",
    "    \n",
    "    def riccati_solver(self, time_grid):\n",
    "        if type(time_grid) == torch.Tensor:\n",
    "            time_grid = time_grid.numpy()\n",
    "\n",
    "        Q_0 = self.R.reshape(4,) # initial condition: Q(0)=S(T)=R\n",
    "\n",
    "        # Solving S(r) on [t,T] is equivalent to solving Q(r)=S(T-r) on [0,T-t] \n",
    "        time_grid_Q = np.flip(self.T-time_grid) \n",
    "        interval = np.array([time_grid_Q[0], time_grid_Q[-1]]) \n",
    "        sol = integrate.solve_ivp(self.riccati_ode, interval, Q_0, t_eval=time_grid_Q)\n",
    "\n",
    "        t_val = self.T - sol.t # do the time-reversal to get the solution S(t)\n",
    "\n",
    "        return np.flip(t_val), np.flip(sol.y)\n",
    "        \n",
    "    def riccati_plot(self, time_grid):\n",
    "        sol_t, sol_y = self.riccati_solver(time_grid)\n",
    "        plt.plot(sol_t,sol_y[0],label='S[0,0]',color='blue')\n",
    "        plt.plot(sol_t,sol_y[1],label='S[0,1]',color='red')\n",
    "        plt.plot(sol_t,sol_y[2],label='S[1,0]',color='yellow')\n",
    "        plt.plot(sol_t,sol_y[3],label='S[1,1]',color='purple')\n",
    "\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('S(t)')\n",
    "        plt.legend(['S[0,0]','S[0,1]','S[1,0]','S[1,1]'])\n",
    "        plt.show()\n",
    "\n",
    "    def value_function(self, t, x):\n",
    "        n = 500 # Fix the number of steps to be 500\n",
    "        val_func = torch.zeros((len(x),1), dtype=torch.float64) \n",
    "        x = x.double()\n",
    "\n",
    "        for j in range(len(x)):\n",
    "            initial_time = t[j].double().item() \n",
    "            step = (self.T-initial_time)/n # step = (T-t)/n\n",
    "            time_grid = torch.arange(initial_time, self.T+step, step) # generate the time grid on [t,T]\n",
    "            t_val, S_r = self.riccati_solver(time_grid)   \n",
    "            S_t = torch.tensor([[S_r[0,0], S_r[1,0]], [S_r[2,0], S_r[3,0]]]) \n",
    "            S_t = S_t.double()\n",
    "\n",
    "            # Assuming sigma is 2x1\n",
    "            sig = torch.matmul(self.sigma, self.sigma.t()) # 2*2 matrix\n",
    "            sig = sig.double()\n",
    "            integral = 0\n",
    "            for i in range(len(t_val)-1):\n",
    "                S_i = torch.tensor([[S_r[0,i], S_r[1,i]], [S_r[2,i], S_r[3,i]]])\n",
    "                S_i_1 = torch.tensor([[S_r[0,i+1], S_r[1,i+1]], [S_r[2,i+1], S_r[3,i+1]]])\n",
    "                difference = S_i_1-S_i\n",
    "                integral += torch.trace(torch.matmul(sig,difference))*(t_val[i+1] - t_val[i])\n",
    "\n",
    "            x_j = x[j].reshape(1,-1).t()\n",
    "            x_j_t = x_j.t()\n",
    "            val_func[j] = torch.linalg.multi_dot([x_j_t,S_t,x_j]) + integral\n",
    "\n",
    "        return val_func\n",
    "        \n",
    "\n",
    "    def optimal_control(self, t, x):\n",
    "        n = 500\n",
    "        a = torch.zeros(len(x), 2)\n",
    "        x = x.double()\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            init_time = t[i].double().item() \n",
    "            step = (self.T-init_time)/n # step = (T-t)/n\n",
    "            time_grid = torch.arange(init_time, self.T+step, step) # generate the time grid on [t,T]\n",
    "            S_r = self.riccati_solver(time_grid)[1]\n",
    "            S_t = torch.tensor([[S_r[0,0], S_r[1,0]], [S_r[2,0], S_r[3,0]]]) \n",
    "            S_t = S_t.double()\n",
    "            x_i = x[i].reshape(1,-1).t() \n",
    "\n",
    "            # The product is 2*1, need to flatten it first before appending the value to a_star\n",
    "            a[i] = -torch.flatten(torch.linalg.multi_dot([self.D,self.M.t(),S_t,x_i])) \n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e62d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1508,  0.0705],\n",
      "        [-0.1754,  0.1189]], grad_fn=<AddmmBackward0>)\n",
      "torch.FloatTensor\n",
      "epoch 99 - lr 0.001 - loss: 19.595554146766663\n",
      "epoch 199 - lr 0.001 - loss: 4.105608345270157\n",
      "epoch 299 - lr 0.001 - loss: 3.2331372701376675\n",
      "epoch 399 - lr 0.001 - loss: 2.1250549018383027\n",
      "epoch 499 - lr 0.001 - loss: 2.038394311517477\n",
      "epoch 599 - lr 0.001 - loss: 1.4185540039278566\n",
      "epoch 699 - lr 0.001 - loss: 1.2136523062363267\n",
      "epoch 799 - lr 0.001 - loss: 0.7073227990511805\n",
      "epoch 899 - lr 0.001 - loss: 0.6233518862724304\n",
      "epoch 999 - lr 0.001 - loss: 1.308500952757895\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 2\n",
    "\n",
    "# Set parameters\n",
    "H = torch.tensor([[1.0,0],[0,1.0]])\n",
    "M = torch.tensor([[1.0,0],[0,1.0]])\n",
    "D = torch.tensor([[0.1,0],[0,0.1]])\n",
    "C = torch.tensor([[0.1,0],[0,0.1]])\n",
    "R = torch.tensor([[1.0,0],[0,1.0]])\n",
    "sigma = torch.tensor([[0.05, 0.05]])\n",
    "\n",
    "# Create data t and x\n",
    "T = 1.0\n",
    "x_range = torch.tensor([-3, 3])\n",
    "y_range = torch.tensor([-3, 3])\n",
    "t = np.random.uniform(0, T, size=batch_size)\n",
    "x = np.random.uniform(-3, 3, size=(batch_size, 1, 2))\n",
    "t0 = torch.from_numpy(np.array([t]).T).float()\n",
    "x0 = torch.from_numpy(x.reshape(batch_size, 2)).float()\n",
    "tx = torch.cat([t0,x0], dim=1)\n",
    "\n",
    "# Convert numpy to torch tensor\n",
    "t = torch.from_numpy(t)\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "# Determine the value function for the samples of t and x\n",
    "lqr_equation = LQR(H, M, D, C, R, sigma, T)\n",
    "opt_control = lqr_equation.optimal_control(t ,x).float()\n",
    "value_func = lqr_equation.value_function(t, x,).float()\n",
    " \n",
    "# Input for FFN neural network (control function)\n",
    "dim = [3,100,100,2] \n",
    "# Input for Net_DGM  neural network (value function)\n",
    "value_dim_input = 2 \n",
    "value_dim_hidden = 100\n",
    "\n",
    "# Input for DGM_Net neural network (PDE)\n",
    "dim_input = 3\n",
    "dim_output = 1\n",
    "num_layers = 3\n",
    "num_neurons = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the control model, loss function, and Adam optimizer\n",
    "control_model = FFN(sizes=dim)\n",
    "# control_loss_fn = nn.MSELoss()\n",
    "control_optimizer = optim.Adam(control_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initailize the value function model, loss function, and Adam optimizer\n",
    "value_model = Net_DGM(value_dim_input, value_dim_hidden)\n",
    "value_optimizer = optim.Adam(value_model.parameters(), lr=learning_rate)\n",
    "\n",
    "alpha_pred = control_model(tx)\n",
    "print(alpha_pred)\n",
    "\n",
    "alpha_pred = alpha_pred.unsqueeze(1).reshape(batch_size,2,1).clone().detach()\n",
    "\n",
    "print(alpha_pred.type())\n",
    "net = DGM_Net(dim_input, dim_output, num_layers, num_neurons)\n",
    "Bellman = Bellman_pde(net, x_range, y_range, H, M, C, D, R, T, sigma, alpha_pred)\n",
    "train = Train(net, Bellman, BATCH_SIZE=batch_size)\n",
    "train.train(epoch=n_epochs, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f6ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e296ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
